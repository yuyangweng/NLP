{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "NLP-RNN-5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuyangweng/NLP/blob/main/NLP_RNN_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsui8SB0t2zl"
      },
      "source": [
        "# # 6-1 加 embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpF4TozkaRNS"
      },
      "source": [
        "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0U7YA9Gt2z3"
      },
      "source": [
        "# RNN寫文章\n",
        "\n",
        "首先，我們預處理數據："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvYSHZ2zHVfn"
      },
      "source": [
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import SimpleRNN\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ7pB8WuKG_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc45d254-9a9b-467a-af7d-493e8595b9dc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE14TUOGI0JV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c208cbd-6b23-42cc-93c4-67c0ff55971d"
      },
      "source": [
        "INPUT_FILE = \"/content/drive/My Drive/00NLP/alice_in_wonderland.txt\"\n",
        "\n",
        "# extract the input as a stream of characters\n",
        "print(\"Extracting text from input...\")\n",
        "fin = open(INPUT_FILE, 'rb')\n",
        "lines = []\n",
        "for line in fin:\n",
        "    line = line.strip().lower()  #刪除空白或結尾符號\n",
        "    line = line.decode(\"ascii\", \"ignore\")\n",
        "    if len(line) == 0:\n",
        "        continue\n",
        "    lines.append(line)\n",
        "fin.close()\n",
        "text = \" \".join(lines) #連成 1 string"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting text from input...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3LV9ROnI3iy"
      },
      "source": [
        "# creating lookup tables\n",
        "# Here chars is the number of features in our character \"vocabulary\"\n",
        "chars = set([c for c in text]) #不重複字母或符號\n",
        "nb_chars = len(chars)\n",
        "char2index = dict((c, i) for i, c in enumerate(chars))\n",
        "index2char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCB0pqQUI38r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b199e507-edd7-429d-eb42-b047cf3c42dd"
      },
      "source": [
        "# create inputs and labels from the text. We do this by stepping\n",
        "# through the text ${step} character at a time, and extracting a \n",
        "# sequence of size ${seqlen} and the next output char. For example,\n",
        "# assuming an input text \"The sky was falling\", we would get the \n",
        "# following sequence of input_chars and label_chars (first 5 only)\n",
        "#   The sky wa -> s\n",
        "#   he sky was ->  \n",
        "#   e sky was  -> f\n",
        "#    sky was f -> a\n",
        "#   sky was fa -> l\n",
        "print(\"Creating input and label text...\")\n",
        "SEQLEN = 10\n",
        "gap = 1\n",
        "\n",
        "input_chars = []\n",
        "label_chars = []\n",
        "for i in range(0, len(text) - SEQLEN, gap):\n",
        "    input_chars.append(text[i:i + SEQLEN]) #每筆輸入10個連續字母或空格\n",
        "    label_chars.append(text[i + SEQLEN]) #對應輸出1個字母或空格"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating input and label text...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWkKLQUSI3x5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54a50a71-e644-4fc0-8a31-f1c5185be76a"
      },
      "source": [
        "# vectorize the input and label chars\n",
        "# Each row of the input is represented by seqlen characters, each \n",
        "# represented as a 1-hot encoding of size len(char). There are \n",
        "# len(input_chars) such rows, so shape(X) is (len(input_chars), seqlen, nb_chars).\n",
        "# 對應(batch_size，timesteps，input_features)輸入\n",
        "#\n",
        "# Each row of output is a single character, also represented as a\n",
        "# dense encoding of size len(char). Hence shape(y) is (len(input_chars),\n",
        "# nb_chars).\n",
        "print(\"Vectorizing input and label text...\")\n",
        "# X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
        "X = np.zeros((len(input_chars), SEQLEN))\n",
        "dataY = []\n",
        "for i, input_char in enumerate(input_chars):#全部10個連續字母或空格之list\n",
        "    for j, ch in enumerate(input_char):#單1組10個連續字母或空格之string\n",
        "        # X[i, j, char2index[ch]] = 1\n",
        "        X[i, j] = char2index[ch] # label encoding\n",
        "    dataY.append(char2index[label_chars[i]])\n",
        "y = utils.to_categorical(np.array(dataY))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorizing input and label text...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tne1uMc6I3YR"
      },
      "source": [
        "# Build the model. We use a single RNN with a fully connected layer\n",
        "# to compute the most likely predicted output char\n",
        "HIDDEN_SIZE = 256\n",
        "BATCH_SIZE = 128\n",
        "NUM_ITERATIONS = 25\n",
        "NUM_EPOCHS_PER_ITERATION = 1\n",
        "NUM_PREDS_PER_EPOCH = 100\n",
        " \n",
        "model = Sequential()\n",
        "model.add(????\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model.add(SimpleRNN(HIDDEN_SIZE, return_sequences=False,\n",
        "#                     input_shape=(SEQLEN, nb_chars)))\n",
        "model.add(Dense(nb_chars))\n",
        "model.add(Activation(\"softmax\"))\n",
        " \n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeTxAOS1KnK0"
      },
      "source": [
        "# We train the model in batches and test output generated at each step\n",
        "for iteration in range(NUM_ITERATIONS):\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Iteration #: %d\" % (iteration))\n",
        "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
        "    \n",
        "    # testing model\n",
        "    # randomly choose a string from input_chars[list of strings], then use it to \n",
        "    # generate text from model for next 100 chars\n",
        "    test_idx = np.random.randint(len(input_chars))\n",
        "    test_chars = input_chars[test_idx]\n",
        "    print(\"Generating from seed: %s\" % (test_chars))\n",
        "    print(test_chars, end=\"\")\n",
        "    for i in range(NUM_PREDS_PER_EPOCH):\n",
        " \n",
        "        Xtest = np.zeros((1, SEQLEN, nb_chars))\n",
        "        for i, ch in enumerate(test_chars):\n",
        "            # Xtest[0, i, char2index[ch]] = 1\n",
        "            Xtest ???\n",
        "\n",
        "\n",
        "        pred = model.predict(Xtest, verbose=0)\n",
        "        ypred = index2char[np.argmax(pred)]\n",
        "        print(ypred, end=\"\")\n",
        "        # move forward with test_chars + ypred\n",
        "        test_chars = test_chars[1:] + ypred\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUKDieItt20H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}